# ==============================================================================
# LLM Gateway - Environment Variables
# ==============================================================================
# Issue 25 Fix (Comp_Static_Analysis_Report_20251203.md):
# This file provides example environment variables for local development.
#
# Setup:
#   1. Copy this file: cp .env.example .env
#   2. Fill in your API keys
#   3. Run: docker-compose up
#
# DO NOT commit .env to version control
# ==============================================================================

# LLM Provider API Keys (required for LLM functionality)
LLM_GATEWAY_ANTHROPIC_API_KEY=your-anthropic-api-key-here
LLM_GATEWAY_OPENAI_API_KEY=your-openai-api-key-here

# Optional: Ollama Configuration (for local models)
# LLM_GATEWAY_OLLAMA_URL=http://localhost:11434

# ==============================================================================
# LlamaCpp Local GGUF Models (Kitchen Brigade Scenario #2)
# ==============================================================================
# Enable local GGUF model inference via llama-cpp-python with Metal acceleration
# Models stored on external flash drive for portability
#
# Requires: CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python
# ==============================================================================
LLM_GATEWAY_LLAMACPP_ENABLED=false
LLM_GATEWAY_LLAMACPP_MODELS_DIR=/Volumes/NO NAME/LLMs/models
LLM_GATEWAY_LLAMACPP_GPU_LAYERS=-1

# Usage:
#   Model names: local/phi-4, local/deepseek-r1-7b, local/qwen2.5-7b
#   Example: curl -X POST http://localhost:8080/v1/chat/completions \
#     -H "Content-Type: application/json" \
#     -d '{"model": "local/phi-4", "messages": [{"role": "user", "content": "Hello"}]}'

# Optional: Override default provider/model
# LLM_GATEWAY_DEFAULT_PROVIDER=anthropic
# LLM_GATEWAY_DEFAULT_MODEL=claude-3-sonnet-20240229

# Optional: Redis password (when using authenticated Redis)
# LLM_GATEWAY_REDIS_PASSWORD=your-redis-password-here
