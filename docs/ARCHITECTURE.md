# LLM Gateway Microservice

> **Version:** 2.0.0  
> **Updated:** 2026-02-01  
> **Status:** Active  
> **Git Reference:** Cross-referenced with codebase 2026-02-01

## Overview

The LLM Gateway is a **microservice** that provides a unified API for LLM interactions. It abstracts provider differences, orchestrates tool-use, manages sessions, and provides operational controls. Multiple applications consume this service over HTTP.

## Architecture Type

**Microservice** - Independently deployable, stateless (sessions in Redis), horizontally scalable.

---

## âš ï¸ Gateway-First Communication Pattern

**CRITICAL RULE**: All external applications MUST route through the Gateway.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVICE COMMUNICATION PATTERN                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  EXTERNAL â†’ PLATFORM: Via Gateway:8080 (REQUIRED)                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  Applications outside the AI Platform must route through Gateway.           â”‚
â”‚                                                                              â”‚
â”‚  âœ… llm-document-enhancer â†’ Gateway:8080 â†’ ai-agents:8082                   â”‚
â”‚  âœ… VS Code Extension â†’ Gateway:8080 â†’ ai-agents:8082                       â”‚
â”‚  âŒ llm-document-enhancer â†’ ai-agents:8082 (VIOLATION!)                     â”‚
â”‚                                                                              â”‚
â”‚  INTERNAL (Platform Services): Direct calls allowed                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚  Platform services (ai-agents, audit-service, Code-Orchestrator,            â”‚
â”‚  semantic-search) may call each other directly.                             â”‚
â”‚                                                                              â”‚
â”‚  âœ… ai-agents:8082 â†’ audit-service:8084 (internal)                          â”‚
â”‚  âœ… ai-agents:8082 â†’ Code-Orchestrator:8083 (internal)                      â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Platform Services (Internal Mesh)

| Service | Port | Direct Access From |
|---------|------|-------------------|
| `llm-gateway` | 8080 | External apps (entry point) |
| `ai-agents` | 8082 | Gateway, platform services |
| `semantic-search-service` | 8081 | Gateway, platform services |
| `Code-Orchestrator-Service` | 8083 | Platform services only |
| `audit-service` | 8084 | Platform services only |
| `inference-service` | 8085 | Gateway only (local LLM inference) |

---

## Kitchen Brigade Role: ROUTER (Pass-Through)

In the Kitchen Brigade architecture, **llm-gateway** is the **Router** - it directs requests but doesn't make content decisions:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ğŸšª ROUTER - TRAFFIC DIRECTOR                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  WHAT IT DOES:                                                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                               â”‚
â”‚  âœ“ Routes LLM requests to appropriate providers (Anthropic, OpenAI, Ollama) â”‚
â”‚  âœ“ Manages chat sessions (in Redis)                                         â”‚
â”‚  âœ“ Registers and executes tools                                             â”‚
â”‚  âœ“ Handles rate limiting, auth, logging                                     â”‚
â”‚  âœ“ Proxies tool calls to other services                                     â”‚
â”‚                                                                              â”‚
â”‚  WHAT IT DOES NOT DO:                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚  âœ— Make decisions about content                                              â”‚
â”‚  âœ— Extract keywords or validate terms                                        â”‚
â”‚  âœ— Host HuggingFace models (that's Code-Orchestrator-Service)               â”‚
â”‚  âœ— Filter or rank search results                                             â”‚
â”‚                                                                              â”‚
â”‚  TOOL EXECUTION:                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚
â”‚  When an LLM requests a tool like `cross_reference`, the gateway:           â”‚
â”‚  1. Receives the tool request from the LLM                                  â”‚
â”‚  2. Proxies to the appropriate service (ai-agents or Code-Orchestrator)     â”‚
â”‚  3. Returns the result to the LLM                                            â”‚
â”‚  The gateway is a pass-through - it doesn't interpret the tool's output.    â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tool Proxy Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LLM Gateway                              â”‚
â”‚                                                                 â”‚
â”‚  Tool Registry:                                                 â”‚
â”‚  â”œâ”€â”€ cross_reference â†’ POST to ai-agents /v1/agents/cross-ref  â”‚
â”‚  â”œâ”€â”€ semantic_search â†’ POST to semantic-search /v1/search      â”‚
â”‚  â”œâ”€â”€ extract_terms   â†’ POST to Code-Orchestrator /api/v1/extractâ”‚
â”‚  â””â”€â”€ ...                                                        â”‚
â”‚                                                                 â”‚
â”‚  The gateway PROXIES these calls - it doesn't execute logic.   â”‚
â”‚  Intelligence lives in the destination services.                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Folder Structure

```
llm-gateway/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                      # FastAPI app entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ deps.py                  # FastAPI dependencies
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py              # POST /v1/chat/completions
â”‚   â”‚   â”‚   â”œâ”€â”€ cms_routing.py       # CMS proxy routes
â”‚   â”‚   â”‚   â”œâ”€â”€ health.py            # /health, /health/detailed, /health/ready
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py            # /v1/models, /v1/providers
â”‚   â”‚   â”‚   â”œâ”€â”€ responses.py         # POST /v1/responses (OpenAI Responses API)
â”‚   â”‚   â”‚   â”œâ”€â”€ sessions.py          # /v1/sessions/*
â”‚   â”‚   â”‚   â””â”€â”€ tools.py             # /v1/tools/*
â”‚   â”‚   â””â”€â”€ middleware/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ logging.py           # Request/response logging
â”‚   â”‚       â”œâ”€â”€ memory.py            # Memory monitoring
â”‚   â”‚       â””â”€â”€ rate_limit.py        # Request rate limiting
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py                # Pydantic settings
â”‚   â”‚   â”œâ”€â”€ exceptions.py            # Custom exceptions
â”‚   â”‚   â””â”€â”€ logging.py               # Structured logging
â”‚   â”‚
â”‚   â”œâ”€â”€ providers/                   # LLM Provider Adapters
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                  # Abstract provider interface
â”‚   â”‚   â”œâ”€â”€ anthropic.py             # Anthropic Claude
â”‚   â”‚   â”œâ”€â”€ deepseek.py              # DeepSeek API
â”‚   â”‚   â”œâ”€â”€ fake.py                  # Test/mock provider
â”‚   â”‚   â”œâ”€â”€ gemini.py                # Google Gemini
â”‚   â”‚   â”œâ”€â”€ inference.py             # Local inference-service
â”‚   â”‚   â”œâ”€â”€ llamacpp.py              # LlamaCpp/GGUF models
â”‚   â”‚   â”œâ”€â”€ ollama.py                # Ollama local
â”‚   â”‚   â”œâ”€â”€ openai.py                # OpenAI GPT
â”‚   â”‚   â”œâ”€â”€ openrouter.py            # OpenRouter (multi-provider)
â”‚   â”‚   â””â”€â”€ router.py                # Provider routing logic
â”‚   â”‚
â”‚   â”œâ”€â”€ clients/                     # Service Clients
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ai_agents.py             # ai-agents service client
â”‚   â”‚   â”œâ”€â”€ circuit_breaker.py       # Circuit breaker pattern
â”‚   â”‚   â”œâ”€â”€ cms_client.py            # Context Management Service client
â”‚   â”‚   â”œâ”€â”€ http.py                  # Base HTTP client
â”‚   â”‚   â””â”€â”€ semantic_search.py       # semantic-search client
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/                       # Tool Execution
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ registry.py              # Tool registration
â”‚   â”‚   â”œâ”€â”€ executor.py              # Tool execution orchestration
â”‚   â”‚   â””â”€â”€ builtin/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ architecture.py      # Architecture tools
â”‚   â”‚       â”œâ”€â”€ chunk_retrieval.py   # Document chunk retrieval
â”‚   â”‚       â”œâ”€â”€ code_orchestrator_tools.py  # Code-Orchestrator proxy
â”‚   â”‚       â”œâ”€â”€ code_review.py       # Code review tools
â”‚   â”‚       â”œâ”€â”€ cross_reference.py   # Cross-reference search
â”‚   â”‚       â”œâ”€â”€ doc_generate.py      # Documentation generation
â”‚   â”‚       â”œâ”€â”€ embed.py             # Embedding tools
â”‚   â”‚       â”œâ”€â”€ enrich_metadata.py   # Metadata enrichment
â”‚   â”‚       â”œâ”€â”€ hybrid_search.py     # Hybrid search proxy
â”‚   â”‚       â””â”€â”€ semantic_search.py   # Semantic search proxy
â”‚   â”‚
â”‚   â”œâ”€â”€ sessions/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ manager.py               # Session lifecycle
â”‚   â”‚   â””â”€â”€ store.py                 # Redis session storage
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cache.py                 # Response caching
â”‚   â”‚   â”œâ”€â”€ chat.py                  # Chat completion business logic
â”‚   â”‚   â””â”€â”€ cost_tracker.py          # Token/cost tracking
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ domain.py                # Domain models (Message, Tool, etc.)
â”‚   â”‚   â”œâ”€â”€ requests.py              # Pydantic request models
â”‚   â”‚   â”œâ”€â”€ responses.py             # Pydantic response models
â”‚   â”‚   â””â”€â”€ tools.py                 # Tool-related models
â”‚   â”‚
â”‚   â”œâ”€â”€ observability/               # Monitoring & Tracing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logging.py               # Structured logging
â”‚   â”‚   â”œâ”€â”€ metrics.py               # Prometheus metrics
â”‚   â”‚   â””â”€â”€ tracing.py               # OpenTelemetry tracing
â”‚   â”‚
â”‚   â””â”€â”€ resilience/                  # Fault Tolerance
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ circuit_breaker_state_machine.py
â”‚       â”œâ”€â”€ fallback_chain.py        # Provider fallback
â”‚       â””â”€â”€ metrics.py               # Resilience metrics
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_providers/
â”‚   â”‚   â”œâ”€â”€ test_tools/
â”‚   â”‚   â””â”€â”€ test_sessions/
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ test_chat_api.py
â”‚   â”‚   â””â”€â”€ test_tool_execution.py
â”‚   â””â”€â”€ conftest.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md              # This file
â”‚   â”œâ”€â”€ TECHNICAL_CHANGE_LOG.md
â”‚   â””â”€â”€ guides/
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docker-compose.dev.yml
â”œâ”€â”€ docker-compose.hybrid.yml
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## System Context

```
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚            CONSUMERS                     â”‚
                          â”‚                                          â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                          â”‚  â”‚ llm-doc-   â”‚  â”‚ ai-agents          â”‚ â”‚
                          â”‚  â”‚ enhancer   â”‚  â”‚ microservice       â”‚ â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                          â”‚        â”‚                   â”‚            â”‚
                          â”‚        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
                          â”‚        â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                          â”‚        â”‚   â”‚  â”‚ Future Apps        â”‚   â”‚
                          â”‚        â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚   â”‚            â”‚
                                   â–¼   â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          LLM GATEWAY MICROSERVICE                             â”‚
â”‚                              (Port 8080)                                      â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                           API Layer (FastAPI)                            â”‚ â”‚
â”‚  â”‚  POST /v1/chat/completions  â”‚  POST /v1/sessions  â”‚  GET /health        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                      â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Provider   â”‚  â”‚   Tool-Use   â”‚  â”‚  â”‚   Session    â”‚  â”‚  Operational â”‚   â”‚
â”‚  â”‚   Router     â”‚  â”‚  Orchestratorâ”‚  â”‚  â”‚   Manager    â”‚  â”‚   Controls   â”‚   â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚  â”‚              â”‚  â”‚              â”‚   â”‚
â”‚  â”‚ â€¢ Anthropic  â”‚  â”‚ â€¢ Registry   â”‚  â”‚  â”‚ â€¢ Create     â”‚  â”‚ â€¢ Rate Limit â”‚   â”‚
â”‚  â”‚ â€¢ OpenAI     â”‚  â”‚ â€¢ Execution  â”‚  â”‚  â”‚ â€¢ Retrieve   â”‚  â”‚ â€¢ Caching    â”‚   â”‚
â”‚  â”‚ â€¢ Ollama     â”‚  â”‚ â€¢ Routing    â”‚  â”‚  â”‚ â€¢ Delete     â”‚  â”‚ â€¢ Cost Track â”‚   â”‚
â”‚  â”‚ â€¢ LlamaCpp   â”‚  â”‚ â€¢ Taxonomy   â”‚  â”‚  â”‚ â€¢ Taxonomy   â”‚  â”‚              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                 â”‚          â”‚         â”‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚          â”‚         â”‚
          â–¼                 â–¼          â”‚         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Providers    â”‚ â”‚ semantic-search â”‚â”‚  â”‚     Redis       â”‚
â”‚                  â”‚ â”‚ microservice    â”‚â”‚  â”‚  (Sessions)     â”‚
â”‚ â€¢ Anthropic API  â”‚ â”‚ (Port 8081)     â”‚â”‚  â”‚                 â”‚
â”‚ â€¢ OpenAI API     â”‚ â”‚                 â”‚â”‚  â”‚                 â”‚
â”‚ â€¢ Ollama (local) â”‚ â”‚                 â”‚â”‚  â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/v1/chat/completions` | LLM inference with optional tool-use |
| POST | `/v1/responses` | OpenAI Responses API compatible endpoint |
| GET | `/v1/models` | List available models |
| GET | `/v1/models/{model_id}` | Get specific model info |
| GET | `/v1/providers` | List available providers |
| POST | `/v1/sessions` | Create new session |
| GET | `/v1/sessions/{id}` | Get session state |
| DELETE | `/v1/sessions/{id}` | Delete session |
| POST | `/v1/tools/execute` | Execute a registered tool |
| GET | `/health` | Health check |
| GET | `/health/detailed` | Detailed health with dependency status |
| GET | `/health/ready` | Readiness check |

---

## Taxonomy-Aware Tool Routing

The LLM Gateway supports taxonomy-aware tool execution. When users specify a taxonomy in their prompt, the gateway passes this to downstream services.

### How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Prompt: "Search for rate limiting patterns, use AI-ML taxonomy"       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  1. LLM Gateway receives chat request                                        â”‚
â”‚  2. LLM decides to call semantic_search tool                                â”‚
â”‚  3. Gateway extracts taxonomy from user context/prompt                       â”‚
â”‚  4. Gateway proxies to semantic-search-service WITH taxonomy parameter:      â”‚
â”‚                                                                              â”‚
â”‚     POST http://semantic-search-service:8081/v1/search/hybrid               â”‚
â”‚     {                                                                        â”‚
â”‚       "query": "rate limiting patterns",                                    â”‚
â”‚       "taxonomy": "AI-ML_taxonomy",    â† Passed from user context           â”‚
â”‚       "tier_filter": [1, 2]            â† Optional tier filter               â”‚
â”‚     }                                                                        â”‚
â”‚                                                                              â”‚
â”‚  5. Results returned with tier/priority from specified taxonomy              â”‚
â”‚  6. LLM uses tier info to prioritize references in response                 â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Session Taxonomy Context

Sessions can store a default taxonomy that applies to all tool calls:

```json
POST /v1/sessions
{
  "context": {
    "taxonomy": "AI-ML_taxonomy",
    "tier_filter": [1, 2, 3]
  }
}
```

This enables users to say "use the Security taxonomy" once, and all subsequent searches in that session use it automatically.

---

## Enrichment Scalability - Gateway Role

The LLM Gateway is a **transparent pass-through** for enriched data. The "compute once, filter at query-time" pattern is fully implemented in semantic-search-service.

### Gateway Does NOT:

| Aspect | Gateway Role |
|--------|--------------|
| Filter `similar_chapters` | âŒ Proxied to semantic-search-service |
| Cache enriched data | âŒ Semantic-search handles caching |
| Apply taxonomy to results | âŒ Done by downstream service |
| Trigger enrichment updates | âŒ CI/CD handles in ai-platform-data |

### Gateway DOES:

| Aspect | Gateway Role |
|--------|--------------|
| Pass `taxonomy` parameter | âœ… Extracted from user prompt/session |
| Pass `tier_filter` parameter | âœ… From session context |
| Proxy to semantic-search | âœ… Transparent routing |
| Return results unchanged | âœ… No interpretation |

### Architecture Compliance

```
User: "Get similar chapters for arch_patterns_ch4, use AI-ML taxonomy"
    â†“
LLM Gateway (Router) - EXTRACTS taxonomy, PROXIES request
    â†“
POST http://semantic-search:8081/v1/search/similar-chapters
{
    "chapter_id": "arch_patterns_ch4",
    "taxonomy": "AI-ML_taxonomy"
}
    â†“
Semantic Search Service - FILTERS similar_chapters by taxonomy
    â†“
{
    "similar_chapters": [...filtered results with tier info...]
}
    â†“
LLM Gateway - RETURNS results unchanged to user
```

The gateway requires **no code changes** to support enrichment scalability. All filtering logic is in semantic-search-service.

---

## Components

### Provider Router
Routes requests to the appropriate LLM provider based on model name or configuration.

### Tool-Use Orchestrator
- Registers available tools
- Parses LLM tool_call responses
- Executes tools (local or proxied to other microservices)
- Returns results to LLM for continuation
- **Passes taxonomy context to downstream services**

### Session Manager
- Creates sessions with TTL
- Stores conversation history
- **Stores taxonomy context per session**
- Uses Redis for distributed session storage

### Operational Controls
- Rate limiting per client
- Response caching
- Token/cost tracking per request

---

## Dependencies

| Dependency | Type | Purpose |
|------------|------|---------|
| Redis | Infrastructure | Session storage, caching |
| semantic-search-service | Microservice | Tool execution for search |
| ai-agents | Microservice | Cross-reference, agent functions |
| Code-Orchestrator | Microservice | Code analysis tools |
| context-management-service | Microservice | Context/session management |
| inference-service | Microservice | Local LLM inference (llamacpp provider) |
| Anthropic API | External | LLM provider (cloud) |
| OpenAI API | External | LLM provider (cloud) |
| Google Gemini API | External | LLM provider (cloud) |
| DeepSeek API | External | LLM provider (cloud) |
| OpenRouter API | External | Multi-provider routing |

---

## Provider Routing

The gateway routes LLM requests to the appropriate provider based on the `model` parameter:

### Supported Providers (11 Total)

| Provider | Models | Target |
|----------|--------|--------|
| `anthropic` | `claude-*` | Anthropic API |
| `openai` | `gpt-*` | OpenAI API |
| `gemini` | `gemini-*` | Google Gemini API |
| `deepseek` | `deepseek-*` | DeepSeek API |
| `openrouter` | Various | OpenRouter (multi-provider) |
| `ollama` | `ollama/*` | Local Ollama server |
| `llamacpp` | `local/*`, GGUF | inference-service:8085 |
| `inference` | Via CMS | inference-service (managed) |
| `fake` | `fake/*` | Test/mock responses |

### Provider Resolution

| Model Pattern | Provider | Target |
|---------------|----------|--------|
| `claude-*`, `anthropic/*` | Anthropic | Anthropic API |
| `gpt-*`, `openai/*` | OpenAI | OpenAI API |
| `ollama/*` | Ollama | Local Ollama server |
| `local/*`, GGUF models | LlamaCpp | inference-service:8085 |

### LlamaCpp Provider (Local Inference)

The `llamacpp` provider routes requests to `inference-service:8085` for local GGUF model inference:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LlamaCpp Provider â†’ Inference Service                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Gateway receives:                                                           â”‚
â”‚  POST /v1/chat/completions                                                  â”‚
â”‚  { "model": "local/phi-4", "messages": [...] }                              â”‚
â”‚                                                                              â”‚
â”‚  Provider Router identifies: model prefix "local/" â†’ LlamaCppProvider       â”‚
â”‚                                                                              â”‚
â”‚  LlamaCppProvider proxies to:                                               â”‚
â”‚  POST http://inference-service:8085/v1/chat/completions                     â”‚
â”‚  { "model": "phi-4", "messages": [...] }                                    â”‚
â”‚                                                                              â”‚
â”‚  Supported models (via inference-service):                                  â”‚
â”‚  - phi-4 (8.4GB)              - deepseek-r1-7b (4.7GB)                      â”‚
â”‚  - qwen2.5-7b (4.5GB)         - llama-3.2-3b (2.0GB)                        â”‚
â”‚  - phi-3-medium-128k (8.6GB)  - granite-8b-code-128k (4.5GB)                â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Deployment

```yaml
# docker-compose.yml
services:
  llm-gateway:
    build: .
    ports:
      - "8080:8080"
    environment:
      - REDIS_URL=redis://redis:6379
      - SEMANTIC_SEARCH_URL=http://semantic-search:8081
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - redis
      - semantic-search

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
```

---

## Service Discovery Patterns

> **WBS 3.2.1.1.4**: Document service discovery patterns for microservice communication.

The LLM Gateway uses **DNS-based service discovery** for communication with dependent services. This pattern is consistent across local development (Docker Compose) and production (Kubernetes).

### Pattern: DNS Service Discovery

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Service Discovery Flow                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚   Environment Variable                     DNS Resolution                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”‚
â”‚                                                                              â”‚
â”‚   LLM_GATEWAY_SEMANTIC_SEARCH_URL          Docker Compose:                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         service name â†’ container IP       â”‚
â”‚   "http://semantic-search:8081"                                              â”‚
â”‚                                            Kubernetes:                       â”‚
â”‚                                            service.namespace.svc.cluster.local â”‚
â”‚                                                                              â”‚
â”‚   LLM_GATEWAY_REDIS_URL                    Both environments:                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    DNS resolves to service endpoint  â”‚
â”‚   "redis://redis:6379"                                                       â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### URL Resolution by Environment

| Environment | Service | URL Pattern | Resolution |
|-------------|---------|-------------|------------|
| Local (direct) | semantic-search | `http://localhost:8081` | Localhost binding |
| Docker Compose | semantic-search | `http://semantic-search:8081` | Docker DNS |
| Kubernetes | semantic-search | `http://semantic-search:8081` | K8s Service DNS |
| Kubernetes (cross-namespace) | semantic-search | `http://semantic-search.default.svc.cluster.local:8081` | FQDN |

### Configuration Hierarchy

```python
# Priority (highest to lowest):
# 1. Environment variable: LLM_GATEWAY_SEMANTIC_SEARCH_URL
# 2. ConfigMap/Secret mount (Kubernetes)
# 3. Default in Settings class: "http://localhost:8081"
```

### Health Check Integration

The gateway's `/health/ready` endpoint verifies connectivity to dependent services:

```
GET /health/ready

Response (all healthy):
{
  "status": "ready",
  "checks": {
    "redis": true,
    "semantic_search": true
  }
}

Response (degraded - semantic-search down):
{
  "status": "degraded",
  "checks": {
    "redis": true,
    "semantic_search": false
  }
}
```

### Graceful Degradation

Following Newman's patterns (Building Microservices pp. 352-353):

1. **Service Unavailable**: Return `503` with `"status": "not_ready"` if critical dependencies down
2. **Degraded Mode**: Return `200` with `"status": "degraded"` if optional services unavailable
3. **Circuit Breaker**: Fast-fail after repeated failures (implemented in `src/clients/circuit_breaker.py`)
4. **Timeout Configuration**: 5-second health check timeout prevents cascading delays

### Docker Compose Example

```yaml
services:
  llm-gateway:
    environment:
      - LLM_GATEWAY_SEMANTIC_SEARCH_URL=http://semantic-search:8081
      - LLM_GATEWAY_REDIS_URL=redis://redis:6379
    depends_on:
      semantic-search:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - app-network

  semantic-search:
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - app-network

networks:
  app-network:
    driver: bridge
```

### Kubernetes ConfigMap Example

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-gateway-config
data:
  LLM_GATEWAY_SEMANTIC_SEARCH_URL: "http://semantic-search:8081"
  LLM_GATEWAY_REDIS_URL: "redis://redis-master:6379"
```

---

## Configuration

```python
# src/core/config.py
class Settings(BaseSettings):
    # Service
    service_name: str = "llm-gateway"
    port: int = 8080
    
    # Redis
    redis_url: str = "redis://localhost:6379"
    
    # Microservice URLs (WBS 3.2.1.1: Service Discovery)
    semantic_search_url: str = "http://localhost:8081"
    
    # Providers
    anthropic_api_key: str
    openai_api_key: str
    default_provider: str = "anthropic"
    default_model: str = "claude-3-sonnet-20240229"
    
    # Rate Limiting
    rate_limit_requests_per_minute: int = 60
    
    class Config:
        env_prefix = "LLM_GATEWAY_"
```
