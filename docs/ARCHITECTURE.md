# LLM Gateway Microservice

## Overview

The LLM Gateway is a **microservice** that provides a unified API for LLM interactions. It abstracts provider differences, orchestrates tool-use, manages sessions, and provides operational controls. Multiple applications consume this service over HTTP.

## Architecture Type

**Microservice** - Independently deployable, stateless (sessions in Redis), horizontally scalable.

---

## âš ï¸ Gateway-First Communication Pattern

**CRITICAL RULE**: All external applications MUST route through the Gateway.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVICE COMMUNICATION PATTERN                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  EXTERNAL â†’ PLATFORM: Via Gateway:8080 (REQUIRED)                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  Applications outside the AI Platform must route through Gateway.           â”‚
â”‚                                                                              â”‚
â”‚  âœ… llm-document-enhancer â†’ Gateway:8080 â†’ ai-agents:8082                   â”‚
â”‚  âœ… VS Code Extension â†’ Gateway:8080 â†’ ai-agents:8082                       â”‚
â”‚  âŒ llm-document-enhancer â†’ ai-agents:8082 (VIOLATION!)                     â”‚
â”‚                                                                              â”‚
â”‚  INTERNAL (Platform Services): Direct calls allowed                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚  Platform services (ai-agents, audit-service, Code-Orchestrator,            â”‚
â”‚  semantic-search) may call each other directly.                             â”‚
â”‚                                                                              â”‚
â”‚  âœ… ai-agents:8082 â†’ audit-service:8084 (internal)                          â”‚
â”‚  âœ… ai-agents:8082 â†’ Code-Orchestrator:8083 (internal)                      â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Platform Services (Internal Mesh)

| Service | Port | Direct Access From |
|---------|------|-------------------|
| `llm-gateway` | 8080 | External apps (entry point) |
| `ai-agents` | 8082 | Gateway, platform services |
| `semantic-search-service` | 8081 | Gateway, platform services |
| `Code-Orchestrator-Service` | 8083 | Platform services only |
| `audit-service` | 8084 | Platform services only |

---

## Kitchen Brigade Role: ROUTER (Pass-Through)

In the Kitchen Brigade architecture, **llm-gateway** is the **Router** - it directs requests but doesn't make content decisions:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ğŸšª ROUTER - TRAFFIC DIRECTOR                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  WHAT IT DOES:                                                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                               â”‚
â”‚  âœ“ Routes LLM requests to appropriate providers (Anthropic, OpenAI, Ollama) â”‚
â”‚  âœ“ Manages chat sessions (in Redis)                                         â”‚
â”‚  âœ“ Registers and executes tools                                             â”‚
â”‚  âœ“ Handles rate limiting, auth, logging                                     â”‚
â”‚  âœ“ Proxies tool calls to other services                                     â”‚
â”‚                                                                              â”‚
â”‚  WHAT IT DOES NOT DO:                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚  âœ— Make decisions about content                                              â”‚
â”‚  âœ— Extract keywords or validate terms                                        â”‚
â”‚  âœ— Host HuggingFace models (that's Code-Orchestrator-Service)               â”‚
â”‚  âœ— Filter or rank search results                                             â”‚
â”‚                                                                              â”‚
â”‚  TOOL EXECUTION:                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚
â”‚  When an LLM requests a tool like `cross_reference`, the gateway:           â”‚
â”‚  1. Receives the tool request from the LLM                                  â”‚
â”‚  2. Proxies to the appropriate service (ai-agents or Code-Orchestrator)     â”‚
â”‚  3. Returns the result to the LLM                                            â”‚
â”‚  The gateway is a pass-through - it doesn't interpret the tool's output.    â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tool Proxy Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LLM Gateway                              â”‚
â”‚                                                                 â”‚
â”‚  Tool Registry:                                                 â”‚
â”‚  â”œâ”€â”€ cross_reference â†’ POST to ai-agents /v1/agents/cross-ref  â”‚
â”‚  â”œâ”€â”€ semantic_search â†’ POST to semantic-search /v1/search      â”‚
â”‚  â”œâ”€â”€ extract_terms   â†’ POST to Code-Orchestrator /api/v1/extractâ”‚
â”‚  â””â”€â”€ ...                                                        â”‚
â”‚                                                                 â”‚
â”‚  The gateway PROXIES these calls - it doesn't execute logic.   â”‚
â”‚  Intelligence lives in the destination services.                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Folder Structure

```
llm-gateway/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py              # POST /v1/chat/completions
â”‚   â”‚   â”‚   â”œâ”€â”€ sessions.py          # /v1/sessions/*
â”‚   â”‚   â”‚   â”œâ”€â”€ tools.py             # /v1/tools/*
â”‚   â”‚   â”‚   â””â”€â”€ health.py            # /health, /ready
â”‚   â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py              # API key validation
â”‚   â”‚   â”‚   â”œâ”€â”€ rate_limit.py        # Request rate limiting
â”‚   â”‚   â”‚   â””â”€â”€ logging.py           # Request/response logging
â”‚   â”‚   â””â”€â”€ deps.py                  # FastAPI dependencies
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py                # Pydantic settings
â”‚   â”‚   â””â”€â”€ exceptions.py            # Custom exceptions
â”‚   â”‚
â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                  # Abstract provider interface
â”‚   â”‚   â”œâ”€â”€ anthropic.py             # Anthropic Claude adapter
â”‚   â”‚   â”œâ”€â”€ openai.py                # OpenAI GPT adapter
â”‚   â”‚   â”œâ”€â”€ ollama.py                # Ollama local adapter
â”‚   â”‚   â””â”€â”€ router.py                # Provider routing logic
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ registry.py              # Tool registration
â”‚   â”‚   â”œâ”€â”€ executor.py              # Tool execution orchestration
â”‚   â”‚   â””â”€â”€ builtin/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ semantic_search.py   # Proxy to semantic-search-service
â”‚   â”‚       â””â”€â”€ chunk_retrieval.py   # Document chunk retrieval
â”‚   â”‚
â”‚   â”œâ”€â”€ sessions/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ manager.py               # Session lifecycle
â”‚   â”‚   â””â”€â”€ store.py                 # Redis session storage
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chat.py                  # Chat completion business logic
â”‚   â”‚   â”œâ”€â”€ cost_tracker.py          # Token/cost tracking
â”‚   â”‚   â””â”€â”€ cache.py                 # Response caching
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ requests.py              # Pydantic request models
â”‚   â”‚   â”œâ”€â”€ responses.py             # Pydantic response models
â”‚   â”‚   â””â”€â”€ domain.py                # Domain models (Message, Tool, etc.)
â”‚   â”‚
â”‚   â””â”€â”€ main.py                      # FastAPI app entry point
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_providers/
â”‚   â”‚   â”œâ”€â”€ test_tools/
â”‚   â”‚   â””â”€â”€ test_sessions/
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ test_chat_api.py
â”‚   â”‚   â””â”€â”€ test_tool_execution.py
â”‚   â””â”€â”€ conftest.py
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ tools.json                   # Tool definitions
â”‚   â””â”€â”€ providers.json               # Provider configurations
â”‚
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ docker/
â”‚   â”‚   â”œâ”€â”€ Dockerfile               # Production multi-stage Dockerfile
â”‚   â”‚   â”œâ”€â”€ Dockerfile.dev           # Development Dockerfile
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml       # Full stack compose
â”‚   â”‚   â”œâ”€â”€ docker-compose.dev.yml   # Dev compose
â”‚   â”‚   â””â”€â”€ .env.example             # Environment template
â”‚   â”œâ”€â”€ kubernetes/
â”‚   â”‚   â”œâ”€â”€ base/                    # Kustomize base manifests
â”‚   â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ overlays/
â”‚   â”‚       â”œâ”€â”€ dev/                 # Dev environment overlay
â”‚   â”‚       â”œâ”€â”€ staging/             # Staging environment overlay
â”‚   â”‚       â””â”€â”€ prod/                # Production environment overlay
â”‚   â””â”€â”€ helm/
â”‚       â””â”€â”€ llm-gateway/             # Helm chart
â”‚           â”œâ”€â”€ Chart.yaml
â”‚           â”œâ”€â”€ values.yaml
â”‚           â”œâ”€â”€ values-dev.yaml
â”‚           â”œâ”€â”€ values-staging.yaml
â”‚           â”œâ”€â”€ values-prod.yaml
â”‚           â”œâ”€â”€ templates/           # Kubernetes templates
â”‚           â””â”€â”€ tests/               # Helm unit tests
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml                   # CI pipeline
â”‚       â”œâ”€â”€ cd-dev.yml               # Dev deployment
â”‚       â”œâ”€â”€ cd-staging.yml           # Staging deployment
â”‚       â””â”€â”€ cd-prod.yml              # Production deployment
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md              # This file
â”‚   â”œâ”€â”€ API.md                       # API documentation
â”‚   â””â”€â”€ DEPLOYMENT.md                # Deployment guide
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ start.sh
â”‚   â””â”€â”€ healthcheck.sh
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## System Context

```
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚            CONSUMERS                     â”‚
                          â”‚                                          â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                          â”‚  â”‚ llm-doc-   â”‚  â”‚ ai-agents          â”‚ â”‚
                          â”‚  â”‚ enhancer   â”‚  â”‚ microservice       â”‚ â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                          â”‚        â”‚                   â”‚            â”‚
                          â”‚        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
                          â”‚        â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                          â”‚        â”‚   â”‚  â”‚ Future Apps        â”‚   â”‚
                          â”‚        â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚   â”‚            â”‚
                                   â–¼   â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          LLM GATEWAY MICROSERVICE                             â”‚
â”‚                              (Port 8080)                                      â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                           API Layer (FastAPI)                            â”‚ â”‚
â”‚  â”‚  POST /v1/chat/completions  â”‚  POST /v1/sessions  â”‚  GET /health        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                      â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Provider   â”‚  â”‚   Tool-Use   â”‚  â”‚  â”‚   Session    â”‚  â”‚  Operational â”‚   â”‚
â”‚  â”‚   Router     â”‚  â”‚  Orchestratorâ”‚  â”‚  â”‚   Manager    â”‚  â”‚   Controls   â”‚   â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚  â”‚              â”‚  â”‚              â”‚   â”‚
â”‚  â”‚ â€¢ Anthropic  â”‚  â”‚ â€¢ Registry   â”‚  â”‚  â”‚ â€¢ Create     â”‚  â”‚ â€¢ Rate Limit â”‚   â”‚
â”‚  â”‚ â€¢ OpenAI     â”‚  â”‚ â€¢ Execution  â”‚  â”‚  â”‚ â€¢ Retrieve   â”‚  â”‚ â€¢ Caching    â”‚   â”‚
â”‚  â”‚ â€¢ Ollama     â”‚  â”‚ â€¢ Routing    â”‚  â”‚  â”‚ â€¢ Delete     â”‚  â”‚ â€¢ Cost Track â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                 â”‚          â”‚         â”‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚          â”‚         â”‚
          â–¼                 â–¼          â”‚         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Providers    â”‚ â”‚ semantic-search â”‚â”‚  â”‚     Redis       â”‚
â”‚                  â”‚ â”‚ microservice    â”‚â”‚  â”‚  (Sessions)     â”‚
â”‚ â€¢ Anthropic API  â”‚ â”‚ (Port 8081)     â”‚â”‚  â”‚                 â”‚
â”‚ â€¢ OpenAI API     â”‚ â”‚                 â”‚â”‚  â”‚                 â”‚
â”‚ â€¢ Ollama (local) â”‚ â”‚                 â”‚â”‚  â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/v1/chat/completions` | LLM inference with optional tool-use |
| POST | `/v1/sessions` | Create new session |
| GET | `/v1/sessions/{id}` | Get session state |
| DELETE | `/v1/sessions/{id}` | Delete session |
| POST | `/v1/tools/execute` | Execute a registered tool |
| GET | `/health` | Health check |
| GET | `/ready` | Readiness check |

---

## Taxonomy-Aware Tool Routing

The LLM Gateway supports taxonomy-aware tool execution. When users specify a taxonomy in their prompt, the gateway passes this to downstream services.

### How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Prompt: "Search for rate limiting patterns, use AI-ML taxonomy"       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  1. LLM Gateway receives chat request                                        â”‚
â”‚  2. LLM decides to call semantic_search tool                                â”‚
â”‚  3. Gateway extracts taxonomy from user context/prompt                       â”‚
â”‚  4. Gateway proxies to semantic-search-service WITH taxonomy parameter:      â”‚
â”‚                                                                              â”‚
â”‚     POST http://semantic-search-service:8081/v1/search/hybrid               â”‚
â”‚     {                                                                        â”‚
â”‚       "query": "rate limiting patterns",                                    â”‚
â”‚       "taxonomy": "AI-ML_taxonomy",    â† Passed from user context           â”‚
â”‚       "tier_filter": [1, 2]            â† Optional tier filter               â”‚
â”‚     }                                                                        â”‚
â”‚                                                                              â”‚
â”‚  5. Results returned with tier/priority from specified taxonomy              â”‚
â”‚  6. LLM uses tier info to prioritize references in response                 â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Session Taxonomy Context

Sessions can store a default taxonomy that applies to all tool calls:

```json
POST /v1/sessions
{
  "context": {
    "taxonomy": "AI-ML_taxonomy",
    "tier_filter": [1, 2, 3]
  }
}
```

This enables users to say "use the Security taxonomy" once, and all subsequent searches in that session use it automatically.

---

## Enrichment Scalability - Gateway Role

The LLM Gateway is a **transparent pass-through** for enriched data. The "compute once, filter at query-time" pattern is fully implemented in semantic-search-service.

### Gateway Does NOT:

| Aspect | Gateway Role |
|--------|--------------|
| Filter `similar_chapters` | âŒ Proxied to semantic-search-service |
| Cache enriched data | âŒ Semantic-search handles caching |
| Apply taxonomy to results | âŒ Done by downstream service |
| Trigger enrichment updates | âŒ CI/CD handles in ai-platform-data |

### Gateway DOES:

| Aspect | Gateway Role |
|--------|--------------|
| Pass `taxonomy` parameter | âœ… Extracted from user prompt/session |
| Pass `tier_filter` parameter | âœ… From session context |
| Proxy to semantic-search | âœ… Transparent routing |
| Return results unchanged | âœ… No interpretation |

### Architecture Compliance

```
User: "Get similar chapters for arch_patterns_ch4, use AI-ML taxonomy"
    â†“
LLM Gateway (Router) - EXTRACTS taxonomy, PROXIES request
    â†“
POST http://semantic-search:8081/v1/search/similar-chapters
{
    "chapter_id": "arch_patterns_ch4",
    "taxonomy": "AI-ML_taxonomy"
}
    â†“
Semantic Search Service - FILTERS similar_chapters by taxonomy
    â†“
{
    "similar_chapters": [...filtered results with tier info...]
}
    â†“
LLM Gateway - RETURNS results unchanged to user
```

The gateway requires **no code changes** to support enrichment scalability. All filtering logic is in semantic-search-service.

---

## Components

### Provider Router
Routes requests to the appropriate LLM provider based on model name or configuration.

### Tool-Use Orchestrator
- Registers available tools
- Parses LLM tool_call responses
- Executes tools (local or proxied to other microservices)
- Returns results to LLM for continuation
- **Passes taxonomy context to downstream services**

### Session Manager
- Creates sessions with TTL
- Stores conversation history
- **Stores taxonomy context per session**
- Uses Redis for distributed session storage

### Operational Controls
- Rate limiting per client
- Response caching
- Token/cost tracking per request

---

## Dependencies

| Dependency | Type | Purpose |
|------------|------|---------|
| Redis | Infrastructure | Session storage, caching |
| semantic-search-service | Microservice | Tool execution for search |
| Anthropic API | External | LLM provider |
| OpenAI API | External | LLM provider |

---

## Deployment

```yaml
# docker-compose.yml
services:
  llm-gateway:
    build: .
    ports:
      - "8080:8080"
    environment:
      - REDIS_URL=redis://redis:6379
      - SEMANTIC_SEARCH_URL=http://semantic-search:8081
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - redis
      - semantic-search

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
```

---

## Service Discovery Patterns

> **WBS 3.2.1.1.4**: Document service discovery patterns for microservice communication.

The LLM Gateway uses **DNS-based service discovery** for communication with dependent services. This pattern is consistent across local development (Docker Compose) and production (Kubernetes).

### Pattern: DNS Service Discovery

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Service Discovery Flow                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚   Environment Variable                     DNS Resolution                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”‚
â”‚                                                                              â”‚
â”‚   LLM_GATEWAY_SEMANTIC_SEARCH_URL          Docker Compose:                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         service name â†’ container IP       â”‚
â”‚   "http://semantic-search:8081"                                              â”‚
â”‚                                            Kubernetes:                       â”‚
â”‚                                            service.namespace.svc.cluster.local â”‚
â”‚                                                                              â”‚
â”‚   LLM_GATEWAY_REDIS_URL                    Both environments:                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    DNS resolves to service endpoint  â”‚
â”‚   "redis://redis:6379"                                                       â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### URL Resolution by Environment

| Environment | Service | URL Pattern | Resolution |
|-------------|---------|-------------|------------|
| Local (direct) | semantic-search | `http://localhost:8081` | Localhost binding |
| Docker Compose | semantic-search | `http://semantic-search:8081` | Docker DNS |
| Kubernetes | semantic-search | `http://semantic-search:8081` | K8s Service DNS |
| Kubernetes (cross-namespace) | semantic-search | `http://semantic-search.default.svc.cluster.local:8081` | FQDN |

### Configuration Hierarchy

```python
# Priority (highest to lowest):
# 1. Environment variable: LLM_GATEWAY_SEMANTIC_SEARCH_URL
# 2. ConfigMap/Secret mount (Kubernetes)
# 3. Default in Settings class: "http://localhost:8081"
```

### Health Check Integration

The gateway's `/health/ready` endpoint verifies connectivity to dependent services:

```
GET /health/ready

Response (all healthy):
{
  "status": "ready",
  "checks": {
    "redis": true,
    "semantic_search": true
  }
}

Response (degraded - semantic-search down):
{
  "status": "degraded",
  "checks": {
    "redis": true,
    "semantic_search": false
  }
}
```

### Graceful Degradation

Following Newman's patterns (Building Microservices pp. 352-353):

1. **Service Unavailable**: Return `503` with `"status": "not_ready"` if critical dependencies down
2. **Degraded Mode**: Return `200` with `"status": "degraded"` if optional services unavailable
3. **Circuit Breaker**: Fast-fail after repeated failures (implemented in `src/clients/circuit_breaker.py`)
4. **Timeout Configuration**: 5-second health check timeout prevents cascading delays

### Docker Compose Example

```yaml
services:
  llm-gateway:
    environment:
      - LLM_GATEWAY_SEMANTIC_SEARCH_URL=http://semantic-search:8081
      - LLM_GATEWAY_REDIS_URL=redis://redis:6379
    depends_on:
      semantic-search:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - app-network

  semantic-search:
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - app-network

networks:
  app-network:
    driver: bridge
```

### Kubernetes ConfigMap Example

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-gateway-config
data:
  LLM_GATEWAY_SEMANTIC_SEARCH_URL: "http://semantic-search:8081"
  LLM_GATEWAY_REDIS_URL: "redis://redis-master:6379"
```

---

## Configuration

```python
# src/core/config.py
class Settings(BaseSettings):
    # Service
    service_name: str = "llm-gateway"
    port: int = 8080
    
    # Redis
    redis_url: str = "redis://localhost:6379"
    
    # Microservice URLs (WBS 3.2.1.1: Service Discovery)
    semantic_search_url: str = "http://localhost:8081"
    
    # Providers
    anthropic_api_key: str
    openai_api_key: str
    default_provider: str = "anthropic"
    default_model: str = "claude-3-sonnet-20240229"
    
    # Rate Limiting
    rate_limit_requests_per_minute: int = 60
    
    class Config:
        env_prefix = "LLM_GATEWAY_"
```
