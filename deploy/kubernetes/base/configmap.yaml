# =============================================================================
# LLM Gateway ConfigMap
# =============================================================================
# Non-sensitive configuration for the LLM Gateway service.
# Sensitive values (API keys) should be stored in Secrets, not here.
#
# Reference: INTEGRATION_MAP.md for service discovery URLs
# =============================================================================

apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-gateway-config
  namespace: llm-services
  labels:
    app.kubernetes.io/name: llm-gateway
    app.kubernetes.io/component: gateway
    app.kubernetes.io/part-of: llm-document-enhancement
    app.kubernetes.io/managed-by: kustomize
data:
  # ===========================================================================
  # Service Configuration
  # ===========================================================================
  LLM_GATEWAY_PORT: "8080"
  LLM_GATEWAY_ENV: "production"
  LLM_GATEWAY_LOG_LEVEL: "INFO"
  LLM_GATEWAY_WORKERS: "4"
  
  # ===========================================================================
  # Service Discovery URLs (per INTEGRATION_MAP.md)
  # ===========================================================================
  # Internal Kubernetes service DNS names
  # Format: <service-name>.<namespace>.svc.cluster.local:<port>
  LLM_GATEWAY_SEMANTIC_SEARCH_URL: "http://semantic-search.llm-services.svc.cluster.local:8081"
  LLM_GATEWAY_AI_AGENTS_URL: "http://ai-agents.llm-services.svc.cluster.local:8082"
  
  # ===========================================================================
  # Redis Configuration
  # ===========================================================================
  # Redis for session storage and caching
  # Using Kubernetes service DNS for Redis
  LLM_GATEWAY_REDIS_URL: "redis://redis.llm-services.svc.cluster.local:6379"
  
  # ===========================================================================
  # LLM Provider Configuration
  # ===========================================================================
  # Default provider: anthropic, openai, or ollama
  LLM_GATEWAY_DEFAULT_PROVIDER: "anthropic"
  LLM_GATEWAY_DEFAULT_MODEL: "claude-3-sonnet-20240229"
  
  # Ollama configuration (for local/self-hosted models)
  LLM_GATEWAY_OLLAMA_BASE_URL: "http://ollama.llm-services.svc.cluster.local:11434"
  
  # ===========================================================================
  # Rate Limiting Configuration
  # ===========================================================================
  LLM_GATEWAY_RATE_LIMIT_REQUESTS_PER_MINUTE: "60"
  LLM_GATEWAY_RATE_LIMIT_TOKENS_PER_MINUTE: "100000"
  
  # ===========================================================================
  # Session Configuration
  # ===========================================================================
  LLM_GATEWAY_SESSION_TTL_SECONDS: "3600"
  LLM_GATEWAY_MAX_CONTEXT_LENGTH: "128000"
  
  # ===========================================================================
  # Health Check Configuration
  # ===========================================================================
  LLM_GATEWAY_HEALTH_CHECK_INTERVAL: "30"
  LLM_GATEWAY_STARTUP_PROBE_DELAY: "10"
